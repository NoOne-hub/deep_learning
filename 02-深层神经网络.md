# 02-深层神经网络



## 深层神经网络

逻辑回归是一个浅层神经网络,严格来说逻辑回归是单层神经网络

算隐层跟输出层, 计算几层神经网络

![image-20200611081924856](imgs/image-20200611081924856.png)

符号标注

![image-20200611081937208](imgs/image-20200611081937208.png)



## 深层网络的前向传播

多层实际也就一个for循环求就行, 4层隐层就用循环1到4

![image-20200611082724177](imgs/image-20200611082724177.png)

## 核对矩阵的维数

这个好算, 我们已知结果层, 假设为3*1 = WX + b

X为(2,1) 则W为(3,2), b为(3,1)

同理可以推到每层矩阵的维度

不知道矩阵维度不好调试

![image-20200611083810647](imgs/image-20200611083810647.png)

![image-20200611083853723](imgs/image-20200611083853723.png)



## 为什么使用深层表示

![image-20200611085426924](imgs/image-20200611085426924.png)

看上图,第一层就是探测边缘,第二层就探测组件了,第三层就可以探测人脸了

如果用单层

![image-20200611085550568](imgs/image-20200611085550568.png)

你看,计算xor电路,单层的话,在右边的话,需要很多的隐层单元

而左边的话,不是按指数级增加隐层单元



## 搭建深层神经网络块

前向传播与反向传播

![image-20200611093143027](imgs/image-20200611093143027.png)

先计算前向传播,然后计算反向传播

![image-20200611093207134](imgs/image-20200611093207134.png)

同时,我们可以缓存Z,0

(W, b)这样在反向传播的时候计算会相对方便



## 前向传播, 反向传播

![image-20200611102536943](imgs/image-20200611102536943.png)

![image-20200611102545629](imgs/image-20200611102545629.png)

![image-20200611102552933](imgs/image-20200611102552933.png)

跟原来的也差不多



## 参数与超参数

- 学习率 α

- 梯度下降循环数量

- 隐层数

- 隐层单元数

- 激活函数个数

这些参数决定了最终得到的W和b

试试修改α,会降低损失函数不, 如果可以就继续, 调参

最优质值是会变的,CPU和GPU都会影响, 可能几个月一变

minibatch size



## 深度学习与神经元关系



简化的神经元

![image-20200611104027791](imgs/image-20200611104027791.png)

突触与下一个神经元的接触

