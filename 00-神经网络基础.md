# 00-神经网络基础

Relu函数



预测房价,我们有很多因素,

我们将这些因素结合起来

![image-20200609221759955](imgs/image-20200609221759955.png)

就得到Y



输入层 全连接



神经网络的应用: 预测广告, 标记照片分类, 语音识别,



## 二分分类

![image-20200609224915711](imgs/image-20200609224915711.png)

单个样本由(x,y)组成



1. 前向传播
2. 反向传播



## 逻辑回归



预测猫若线性回归, 线性回归不满足0或1

![image-20200610100951177](imgs/image-20200610100951177.png)

所以用sigmod函数加上,这样就在0-1之间

这样就是满足要么0, 要么1

通常将 w和b分开



### 代价函数

Lost function



![image-20200610101810539](imgs/image-20200610101810539.png)

Cost function 

![image-20200610102100831](imgs/image-20200610102100831.png)

找W和b来缩小J

## 梯度下降法

损失函数可以衡量训练集上的效果



梯度下将就是往下降最快的方向走, 就是求导数,然后往反的来,增长最快的反方向

α为学习率

w := w - α * dw

![image-20200610102838872](imgs/image-20200610102838872.png)

左边导数为负数,所以减去后就变+了,就往中间走,右边导数为正数, -了就一样往中间走,为的就是招最低点, 同时步长也要选好, 下降太快找不到最低点,下降太慢需要太长时间, 所以需要选好α

![image-20200610103044465](imgs/image-20200610103044465.png)



## 微积分和导数

![image-20200610103515518](imgs/image-20200610103515518.png)

斜率说明, 实际就是导数, f(a) 求导= 3

增加横坐标后,y坐标增加为横坐标的增加的数值的3倍

![image-20200610103913095](imgs/image-20200610103913095.png)

这是我看错,还是他算错了, 应该为4.002, 因为f(a) ' = 2a



## 计算图

类似复合函数求导

正向传播

![image-20200610104205286](imgs/image-20200610104205286.png)

反向传播

![image-20200610104630320](imgs/image-20200610104630320.png)

链式法则

![image-20200610110434715](imgs/image-20200610110434715.png)

也不是很难理解, dj/dv=3, 然后一直推

dj/da = dj/dv * dv/da= 3 * 1 =3

dj/du = dj/dv * dv/du = 3*1 =3

dj/db = dj/du * du/db = 3*2 =6

dj/dc = dj/du * du/dc = 3*3 = 9



## 逻辑回归中的梯度下降法

![image-20200610110942333](imgs/image-20200610110942333.png)

修改w和b来减小损失函数

![image-20200610111129947](imgs/image-20200610111129947.png)

往前求,最后计算出每个参数对L的影响

![image-20200610111219846](imgs/image-20200610111219846.png)

求出后,就可以用梯度下降法计算了



## m个样本的逻辑回归



![image-20200610141149221](imgs/image-20200610141149221.png)

引入矢量化技术, 摆脱for循环



## 向量化



np.dot(W,x)

w.T.dot(x)

将循环转成矩阵

![image-20200610184353937](imgs/image-20200610184353937.png)



## 向量化逻辑回归

![image-20200610185205696](imgs/image-20200610185205696.png)



![image-20200610185400587](imgs/image-20200610185400587.png)

![image-20200610185609811](imgs/image-20200610185609811.png)



![image-20200610185801465](imgs/image-20200610185801465.png)

这里用矩阵简化了很多操作,循环

,最后需要一个循环来迭代次数, 梯度下降的次数



## 广播

axis=0 沿垂直方向求和



cal =A.sum(asix=0)

reshape矩阵形状 O(1)时间复杂度

![image-20200610190533951](imgs/image-20200610190533951.png)



广播例子2

![image-20200610190627856](imgs/image-20200610190627856.png)

将100自动扩充

广播例子3

![image-20200610190713724](imgs/image-20200610190713724.png)

通用广播规则:

(m,n) + (1,n) =>(m,n) +(m, n)

同样的一个数也会被扩充成行向量或者列向量



不要使用未声明的秩, 比如(5,) (,5) 这种,而使用(5,1) 这种显示声明的

![image-20200610191423487](imgs/image-20200610191423487.png)

利用断点判断



